"""
READY-TO-RUN PDF Q&A SYSTEM
Just add your API key and PDF path, then run!

Installation:
pip install openai langchain langchain-openai pypdf faiss-cpu
"""

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.document_loaders import PyPDFLoader
from langchain.prompts import PromptTemplate

# ========================================
# CONFIGURATION - CHANGE THESE VALUES
# ========================================

# 1. Put your OpenAI API key here
OPENAI_API_KEY = "sk-proj-xxxxxxxxxxxxxxxxxxxxx"  # â† REPLACE THIS

# 2. Put your PDF file path here
PDF_PATH = "your_document.pdf"  # â† REPLACE THIS

# 3. Your questions (add more if you want)
QUESTIONS = [
    "What is the main topic of this document?",
    "Can you provide a summary?",
    "What are the key findings or conclusions?"
]

# ========================================
# NO NEED TO CHANGE ANYTHING BELOW
# ========================================

def main():
    print("=" * 70)
    print("ðŸ“„ PDF QUESTION & ANSWER SYSTEM")
    print("=" * 70)
    
    # Validate inputs
    if "xxxxx" in OPENAI_API_KEY:
        print("\nâŒ ERROR: Please add your OpenAI API key in the code!")
        print("Look for: OPENAI_API_KEY = \"sk-proj-xxxxx...\"")
        return
    
    if PDF_PATH == "your_document.pdf":
        print("\nâŒ ERROR: Please add your PDF file path in the code!")
        print("Look for: PDF_PATH = \"your_document.pdf\"")
        return
    
    try:
        # Step 1: Load PDF
        print(f"\nðŸ“‚ Loading PDF: {PDF_PATH}")
        loader = PyPDFLoader(PDF_PATH)
        documents = loader.load()
        print(f"   âœ… Loaded {len(documents)} pages")
        
        # Step 2: Split into chunks
        print("\nâœ‚ï¸  Splitting document into chunks...")
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len,
        )
        texts = text_splitter.split_documents(documents)
        print(f"   âœ… Created {len(texts)} chunks")
        
        # Step 3: Create embeddings
        print("\nðŸ”„ Creating embeddings (this may take a moment)...")
        embeddings = OpenAIEmbeddings(
            openai_api_key=OPENAI_API_KEY,
            model="text-embedding-3-small"
        )
        
        # Step 4: Create vector store
        print("   Creating vector database...")
        vector_store = FAISS.from_documents(texts, embeddings)
        print("   âœ… Vector database created")
        
        # Step 5: Create LLM
        print("\nðŸ¤– Initializing GPT-4...")
        llm = ChatOpenAI(
            openai_api_key=OPENAI_API_KEY,
            model="gpt-4",
            temperature=0
        )
        print("   âœ… LLM ready")
        
        # Step 6: Create QA chain
        prompt_template = """Use the following context to answer the question. 
        If you don't know the answer, say so clearly.

        Context: {context}

        Question: {question}

        Answer:"""
        
        PROMPT = PromptTemplate(
            template=prompt_template,
            input_variables=["context", "question"]
        )
        
        qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=vector_store.as_retriever(search_kwargs={"k": 4}),
            return_source_documents=True,
            chain_type_kwargs={"prompt": PROMPT}
        )
        
        print("\n" + "=" * 70)
        print("ðŸŽ‰ SYSTEM READY! Answering your questions...")
        print("=" * 70)
        
        # Step 7: Answer questions
        for i, question in enumerate(QUESTIONS, 1):
            print(f"\n\n{'â”€' * 70}")
            print(f"â“ QUESTION {i}: {question}")
            print('â”€' * 70)
            
            result = qa_chain({"query": question})
            answer = result["result"]
            
            print(f"\nðŸ’¡ ANSWER:\n{answer}")
            
            # Show sources
            sources = result["source_documents"]
            print(f"\nðŸ“š Based on {len(sources)} source chunks from the document")
        
        print("\n\n" + "=" * 70)
        print("âœ… ALL DONE!")
        print("=" * 70)
        
        # Interactive mode
        print("\nðŸ’¬ Want to ask more questions? (Enter 'quit' to exit)")
        while True:
            user_question = input("\nYour question: ").strip()
            if user_question.lower() in ['quit', 'exit', 'q']:
                print("\nðŸ‘‹ Goodbye!")
                break
            
            if user_question:
                print("\nðŸ¤” Thinking...")
                result = qa_chain({"query": user_question})
                print(f"\nðŸ’¡ Answer:\n{result['result']}")
        
    except FileNotFoundError:
        print(f"\nâŒ ERROR: PDF file not found at: {PDF_PATH}")
        print("Please check the file path and try again.")
    
    except Exception as e:
        print(f"\nâŒ ERROR: {str(e)}")
        print("\nðŸ” Possible issues:")
        print("1. Check your API key is correct")
        print("2. Make sure you have credits in OpenAI account")
        print("3. Verify PDF file exists at the specified path")
        print("4. Check your internet connection")

if __name__ == "__main__":
    main()












# Test Case Generator - Setup Guide

## Installation

### 1. Install Dependencies

```bash
pip install fastapi uvicorn python-multipart
pip install PyPDF2 python-docx python-pptx openpyxl
pip install langchain langchain-community langchain-openai
pip install faiss-cpu
pip install pydantic
```

### 2. Environment Variables

Create a `.env` file in your project root:

```env
AZURE_OPENAI_KEY=your_azure_openai_api_key
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
AZURE_DEPLOYMENT_NAME=gpt-4
AZURE_EMBEDDINGS_DEPLOYMENT=text-embedding-3-small
```

### 3. Azure OpenAI Setup

1. Create an Azure OpenAI resource
2. Deploy a GPT-4 model (or update `AZURE_DEPLOYMENT_NAME` to your model)
3. Deploy a text-embedding model (e.g., text-embedding-3-small)
4. Copy your API key and endpoint to `.env`

## Running the Server

```bash
python main.py
```

Server runs on `http://localhost:8000`

## API Endpoints

### 1. Upload Document & Generate Test Cases

**Endpoint:** `POST /upload-and-generate`

**Parameters:**
- `file` (multipart/form-data): Document file (PDF, DOCX, PPT, XLSX)
- `num_test_cases` (optional, default=5): Number of test cases to generate
- `query` (optional): Custom query for test case generation

**Example:**
```bash
curl -X POST "http://localhost:8000/upload-and-generate" \
  -F "file=@document.pdf" \
  -F "num_test_cases=5" \
  -F "query=Generate test cases for login functionality"
```

**Response:**
```json
{
  "status": "success",
  "file_name": "document.pdf",
  "document_type": "PDF",
  "chunks_created": 12,
  "test_cases_generated": 5,
  "test_cases": [
    {
      "test_case_id": "TC_001",
      "test_case_description": "Verify valid user login",
      "prerequisites": ["User account exists", "Application is accessible"],
      "test_steps": ["Open login page", "Enter credentials", "Click login"],
      "test_data": {"username": "test@example.com", "password": "password123"},
      "expected_result": "User logged in successfully",
      "actual_result": "",
      "status": "Not Executed",
      "created_by": "QA Team",
      "date_of_creation": "2025-11-14",
      "executed_by": "",
      "date_of_execution": "",
      "page_number": "1"
    }
  ]
}
```

### 2. Generate from Text

**Endpoint:** `POST /generate-from-text`

**Parameters:**
- `text_content`: Raw text content
- `num_test_cases` (optional, default=5): Number of test cases
- `query` (optional): Custom query

**Example:**
```bash
curl -X POST "http://localhost:8000/generate-from-text" \
  -H "Content-Type: application/json" \
  -d '{
    "text_content": "The system should allow users to reset their password...",
    "num_test_cases": 3
  }'
```

### 3. Health Check

**Endpoint:** `GET /health`

```bash
curl http://localhost:8000/health
```

## How It Works

### Document Processing
1. Uploads document (PDF, DOCX, PPT, XLSX)
2. Extracts text content with page/slide references
3. Splits text into chunks (500 characters, 100 overlap)

### RAG with Reranking
1. Creates vector embeddings using Azure OpenAI Embeddings
2. Stores in FAISS vector database
3. Retrieves 10 initial relevant chunks
4. **Reranks top 5** using LLMListwiseReranker for better relevance

### Test Case Generation
1. Uses reranked context
2. Calls Azure OpenAI GPT-4 to generate structured test cases
3. Returns JSON with all required fields:
   - Test Case ID
   - Description
   - Prerequisites
   - Test Steps
   - Test Data
   - Expected Result
   - Status tracking fields

## Features

âœ… **Multi-format Support**: PDF, DOCX, PPT, XLSX  
âœ… **RAG with Reranking**: Better context retrieval quality  
âœ… **Azure OpenAI Integration**: Uses your Azure credentials  
âœ… **Structured Output**: JSON format for easy integration  
âœ… **Page Tracking**: References document source (page/slide numbers)  
âœ… **Comprehensive Test Cases**: Covers normal paths, edge cases, error handling  

## Docker Deployment

Create a `Dockerfile`:

```dockerfile
FROM python:3.11-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install -r requirements.txt

COPY main.py .

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

Build and run:
```bash
docker build -t testcase-generator .
docker run -p 8000:8000 \
  -e AZURE_OPENAI_KEY=your_key \
  -e AZURE_OPENAI_ENDPOINT=your_endpoint \
  -e AZURE_DEPLOYMENT_NAME=gpt-4 \
  testcase-generator
```

## Troubleshooting

**Issue:** "Invalid API key"
- Verify Azure credentials in `.env`
- Check deployment names match your Azure resources

**Issue:** "No text extracted"
- Ensure document has selectable text (not scanned image)
- Try converting to compatible format

**Issue:** Poor test case quality
- Adjust chunk size in `create_rag_retriever()` function
- Modify the prompt template for specific requirements
- Increase `top_n` in reranker for more context


"""
Test Case Generator with RAG and Reranking using FastAPI
Supports: PDF, DOCX, PPT, Excel documents
Uses Azure OpenAI and LangChain with reranking
"""

from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
import os
import json
from typing import List
from datetime import datetime
import io

# Document processing
from PyPDF2 import PdfReader
from docx import Document
from pptx import Presentation
import openpyxl

# LangChain and embeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import AzureOpenAIEmbeddings
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMListwiseReranker
from langchain_openai import AzureChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# Initialize FastAPI
app = FastAPI(title="Test Case Generator API", version="1.0.0")

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Azure OpenAI Configuration
AZURE_OPENAI_KEY = os.getenv("AZURE_OPENAI_KEY", "your-key-here")
AZURE_OPENAI_ENDPOINT = os.getenv("AZURE_OPENAI_ENDPOINT", "your-endpoint-here")
AZURE_DEPLOYMENT_NAME = os.getenv("AZURE_DEPLOYMENT_NAME", "gpt-4")
AZURE_EMBEDDINGS_DEPLOYMENT = os.getenv("AZURE_EMBEDDINGS_DEPLOYMENT", "text-embedding-3-small")
API_VERSION = "2024-02-15-preview"

# Initialize Azure OpenAI
embeddings = AzureOpenAIEmbeddings(
    azure_deployment=AZURE_EMBEDDINGS_DEPLOYMENT,
    openai_api_version=API_VERSION,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    api_key=AZURE_OPENAI_KEY,
)

llm = AzureChatOpenAI(
    azure_deployment=AZURE_DEPLOYMENT_NAME,
    openai_api_version=API_VERSION,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    api_key=AZURE_OPENAI_KEY,
    temperature=0.7,
)

# Document extraction functions
def extract_pdf_text(file_content: bytes) -> str:
    """Extract text from PDF"""
    pdf_file = io.BytesIO(file_content)
    pdf_reader = PdfReader(pdf_file)
    text = ""
    for page_num, page in enumerate(pdf_reader.pages):
        text += f"\n--- Page {page_num + 1} ---\n"
        text += page.extract_text()
    return text

def extract_docx_text(file_content: bytes) -> str:
    """Extract text from Word document"""
    doc_file = io.BytesIO(file_content)
    doc = Document(doc_file)
    text = ""
    for para in doc.paragraphs:
        text += para.text + "\n"
    for table in doc.tables:
        for row in table.rows:
            for cell in row.cells:
                text += cell.text + " | "
            text += "\n"
    return text

def extract_ppt_text(file_content: bytes) -> str:
    """Extract text from PowerPoint"""
    ppt_file = io.BytesIO(file_content)
    prs = Presentation(ppt_file)
    text = ""
    for slide_num, slide in enumerate(prs.slides):
        text += f"\n--- Slide {slide_num + 1} ---\n"
        for shape in slide.shapes:
            if hasattr(shape, "text"):
                text += shape.text + "\n"
    return text

def extract_excel_text(file_content: bytes) -> str:
    """Extract text from Excel"""
    excel_file = io.BytesIO(file_content)
    wb = openpyxl.load_workbook(excel_file)
    text = ""
    for sheet_name in wb.sheetnames:
        ws = wb[sheet_name]
        text += f"\n--- Sheet: {sheet_name} ---\n"
        for row in ws.iter_rows(values_only=True):
            text += " | ".join([str(cell) if cell else "" for cell in row]) + "\n"
    return text

def extract_document_text(file: UploadFile, file_content: bytes) -> tuple[str, str]:
    """Extract text based on file type"""
    filename = file.filename.lower()

    if filename.endswith('.pdf'):
        text = extract_pdf_text(file_content)
        page_info = "PDF"
    elif filename.endswith('.docx'):
        text = extract_docx_text(file_content)
        page_info = "DOCX"
    elif filename.endswith(('.ppt', '.pptx')):
        text = extract_ppt_text(file_content)
        page_info = "PPT"
    elif filename.endswith(('.xlsx', '.xls')):
        text = extract_excel_text(file_content)
        page_info = "Excel"
    else:
        raise HTTPException(status_code=400, detail="Unsupported file format")

    return text, page_info

def create_rag_retriever(documents_text: str, chunk_size: int = 500, chunk_overlap: int = 100):
    """Create RAG retriever with reranking"""
    # Split documents into chunks
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        separators=["\n\n", "\n", " ", ""]
    )
    chunks = splitter.split_text(documents_text)

    # Create vector store
    vectorstore = FAISS.from_texts(chunks, embeddings)
    base_retriever = vectorstore.as_retriever(search_kwargs={"k": 10})

    # Add reranking
    compressor = LLMListwiseReranker(
        llm=llm,
        top_n=5
    )
    retriever = ContextualCompressionRetriever(
        base_compressor=compressor,
        base_retriever=base_retriever
    )

    return retriever, chunks

def generate_test_cases(retriever, query: str, num_cases: int = 5) -> List[dict]:
    """Generate test cases from retrieved documents"""

    # Retrieve relevant context
    relevant_docs = retriever.invoke(query)
    context = "\n".join([doc.page_content for doc in relevant_docs])

    # Prompt for test case generation
    prompt_template = PromptTemplate(
        input_variables=["context", "num_cases"],
        template="""Based on the following document context, generate {num_cases} comprehensive test cases.

Document Context:
{context}

Generate test cases with the following structure for each one. Return as JSON array:
[
  {{
    "test_case_id": "TC_001",
    "test_case_description": "Brief description",
    "prerequisites": ["prerequisite 1", "prerequisite 2"],
    "test_steps": ["step 1", "step 2", "step 3"],
    "test_data": {{"key": "value"}},
    "expected_result": "Description of expected result",
    "actual_result": "",
    "status": "Not Executed",
    "created_by": "QA Team",
    "date_of_creation": "{date}",
    "executed_by": "",
    "date_of_execution": "",
    "page_number": "1"
  }}
]

Ensure test cases cover:
1. Normal/Happy path scenarios
2. Edge cases
3. Error handling
4. Business logic validation
5. Data validation

Return ONLY valid JSON array, no additional text.""".format(date=datetime.now().strftime("%Y-%m-%d"), num_cases=num_cases)
    )

    chain = LLMChain(llm=llm, prompt=prompt_template)
    response = chain.invoke({"context": context, "num_cases": num_cases})

    # Parse response
    try:
        # Extract JSON from response
        response_text = response.get("text", "")
        json_start = response_text.find("[")
        json_end = response_text.rfind("]") + 1
        if json_start != -1 and json_end > json_start:
            json_str = response_text[json_start:json_end]
            test_cases = json.loads(json_str)
        else:
            test_cases = []
    except json.JSONDecodeError:
        test_cases = []

    return test_cases

@app.post("/upload-and-generate")
async def upload_and_generate(
    file: UploadFile = File(...),
    num_test_cases: int = 5,
    query: str = "Generate test cases for the functionality described"
):
    """
    Upload a document and generate test cases

    Args:
        file: Document file (PDF, DOCX, PPT, XLSX)
        num_test_cases: Number of test cases to generate
        query: Custom query for test case generation
    """
    try:
        # Read file content
        file_content = await file.read()

        # Extract text
        document_text, doc_type = extract_document_text(file, file_content)

        if not document_text.strip():
            raise HTTPException(status_code=400, detail="No text could be extracted from document")

        # Create RAG retriever with reranking
        retriever, chunks = create_rag_retriever(document_text)

        # Generate test cases
        test_cases = generate_test_cases(retriever, query, num_test_cases)

        return JSONResponse({
            "status": "success",
            "file_name": file.filename,
            "document_type": doc_type,
            "chunks_created": len(chunks),
            "test_cases_generated": len(test_cases),
            "test_cases": test_cases
        })

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error processing file: {str(e)}")

@app.post("/generate-from-text")
async def generate_from_text(
    text_content: str,
    num_test_cases: int = 5,
    query: str = "Generate comprehensive test cases"
):
    """Generate test cases from raw text input"""
    try:
        if not text_content.strip():
            raise HTTPException(status_code=400, detail="Text content cannot be empty")

        # Create RAG retriever
        retriever, chunks = create_rag_retriever(text_content)

        # Generate test cases
        test_cases = generate_test_cases(retriever, query, num_test_cases)

        return JSONResponse({
            "status": "success",
            "chunks_created": len(chunks),
            "test_cases_generated": len(test_cases),
            "test_cases": test_cases
        })

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error: {str(e)}")

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return JSONResponse({"status": "healthy", "service": "Test Case Generator API"})

@app.get("/")
async def root():
    """Root endpoint with API documentation"""
    return JSONResponse({
        "service": "Test Case Generator API",
        "version": "1.0.0",
        "endpoints": {
            "POST /upload-and-generate": "Upload document and generate test cases",
            "POST /generate-from-text": "Generate test cases from text",
            "GET /health": "Health check"
        },
        "supported_formats": ["PDF", "DOCX", "PPT", "XLSX", "XLS"]
    })

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
